{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GzbXIfc6t3nr"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "CRAWL_PROFILE = True\n",
    "CRAWL_ARTICLE = True\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLmKSDpzt3nt",
    "outputId": "07a06284-cbfb-47df-fd1d-f68b6dc09cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  2 15:25:28 2023\n"
     ]
    }
   ],
   "source": [
    "seed_url='https://pureportal.coventry.ac.uk/en/organisations/school-of-computing-mathematics-and-data-sciences/persons/'\n",
    "# seed_url='https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/'\n",
    "base_url='https://pureportal.coventry.ac.uk/en'\n",
    "articles_inputFile = \"CoventryUni_Data_Scraped_articles.csv\"\n",
    "\n",
    "queue= [seed_url]\n",
    "already_visited=[]\n",
    "total_entries= 0\n",
    "last_page_entries =0\n",
    "profile_id = 1\n",
    "document_id =1\n",
    "data_table = {}\n",
    "article_table = {}\n",
    "\n",
    "crawl_date = time.ctime()\n",
    "print(crawl_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEfwtpIOt3nu"
   },
   "source": [
    "# Save crawled data into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LzB8Cuxgt3nv"
   },
   "outputs": [],
   "source": [
    "#Write Profile Data into CSV file\n",
    "def writeData_profile():\n",
    "    global data_table\n",
    "    col_names=['Profile_Name','Profile_Link','Designation']\n",
    "\n",
    "    data_frame=pd.DataFrame.from_dict(data_table,orient ='index',columns=col_names)\n",
    "    data_frame.index.rename('Profile_Id', inplace=True)\n",
    "    \n",
    "    file_exists = os.path.isfile(\"CoventryUni_Data_Scraped_profile.csv\")\n",
    "    \n",
    "    print(file_exists)\n",
    "    if file_exists: #backup the file\n",
    "        shutil.copy(\"CoventryUni_Data_Scraped_profile.csv\", \"backup/CoventryUni_Data_Scraped_profile_last.csv\")  \n",
    "        \n",
    "    data_frame.to_csv(\"CoventryUni_Data_Scraped_profile.csv\")\n",
    "\n",
    "\n",
    "#Write Article Data into CSV file\n",
    "def writeData_articles():\n",
    "    global article_table\n",
    "    col_names=['Profile_Name','Title','Title_Link','Year_Published']\n",
    "\n",
    "    data_frame=pd.DataFrame.from_dict(article_table,orient='index',columns=col_names)\n",
    "    data_frame.index.rename('Document_Id', inplace=True)\n",
    "    \n",
    "    file_exists = os.path.isfile(\"CoventryUni_Data_Scraped_articles.csv\")\n",
    "    \n",
    "    print(file_exists)\n",
    "    if file_exists: #backup the file\n",
    "        shutil.copy(\"CoventryUni_Data_Scraped_articles.csv\", \"backup/CoventryUni_Data_Scraped_articles_last.csv\") \n",
    "        \n",
    "    data_frame.to_csv(\"CoventryUni_Data_Scraped_articles.csv\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYsW-DBxt3nw"
   },
   "source": [
    "# Extract Next page link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WkxL2HWxt3nx"
   },
   "outputs": [],
   "source": [
    "# returns link of next page\n",
    "def extract_next_page_link(soup):\n",
    "    global seed_url\n",
    "    global total_entries\n",
    "    #find the html tags for next page from html page\n",
    "    next_link = soup.find('a', attrs={'class': 'nextLink'})\n",
    "    href = next_link.get('href')\n",
    "    page_number = href.split('=')[-1]\n",
    "\n",
    "    # extract the keys for next page - \n",
    "    next_page_link = seed_url+f'?page={page_number}'\n",
    "\n",
    "#     print(next_page_keys)\n",
    "    return next_page_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZbQXkDct3nx"
   },
   "source": [
    "# Find Profile name link and designation from Profile Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TGUIljdLt3nx"
   },
   "outputs": [],
   "source": [
    "# Update profile information in the global data_table\n",
    "def extract_info_from_page(page):\n",
    "    global base_url\n",
    "    global total_entries\n",
    "    global queue\n",
    "    global last_page_entries\n",
    "    global data_table\n",
    "    global profile_id\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    users= soup.find_all('li',attrs={'class':'grid-result-item'})\n",
    "    # print(\"USERS-----------------------------\", len(users))\n",
    "    no_of_entries_in_page = 0\n",
    "    for user in users:\n",
    "        try:\n",
    "          profile_name=user.find('h3',attrs={'class':'title'})\n",
    "          # print(\"pname--------------\", profile_name)\n",
    "        except:\n",
    "          profile_name = False\n",
    "          pass\n",
    "        try:\n",
    "          link = user.find('a')['href']\n",
    "          linkp = link+'/publications/'\n",
    "          # print(\"link--------------\", linkp)\n",
    "        except:\n",
    "          linkp = False\n",
    "          pass\n",
    "        try:\n",
    "          designation = user.find('span',attrs={'class':'minor'}).text\n",
    "          # print(\"designation--------------\", designation)\n",
    "        except:\n",
    "          designation = False\n",
    "          pass\n",
    "\n",
    "        no_of_entries_in_page +=1\n",
    "        \n",
    "        data_table[profile_id] = [profile_name.text,linkp,designation]\n",
    "        \n",
    "        profile_id = profile_id+1\n",
    "        \n",
    "        # print(\"User Name: \",profile_name.text)\n",
    "        # print(\"Link : \",base_url+link)\n",
    "        # print(\"Designation :\",designation.text)\n",
    "        # print(\"\\n\")\n",
    "\n",
    "    if last_page_entries == 0:\n",
    "        last_page_entries = no_of_entries_in_page\n",
    "\n",
    "    if no_of_entries_in_page < last_page_entries : # We have reached to last page\n",
    "        total_entries=total_entries+no_of_entries_in_page\n",
    "    else:    \n",
    "        total_entries=total_entries+no_of_entries_in_page\n",
    "        next_page= extract_next_page_link(soup) # still more pages left\n",
    "        queue.append(next_page)\n",
    "        last_page_entries = no_of_entries_in_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL_KXIgFt3ny"
   },
   "source": [
    "# Extract individual Profile such as title research paper link and published year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LG6nEGktt3nz"
   },
   "outputs": [],
   "source": [
    "# Input - Profile link\n",
    "# output - update global article_table with all the details related with the profile \n",
    "def extract_user_info(profile_url):\n",
    "    global article_table\n",
    "    global document_id\n",
    "    start = 0\n",
    "#     page_size=20    \n",
    "    #access url of profile \n",
    "    page =requests.get(profile_url)   \n",
    "    \n",
    "    if page.status_code != 200:\n",
    "        print(\"Failed to access url..[ERROR_CODE]:\", page.status_code)\n",
    "        print(\"Page Url : \",profile_url)\n",
    "        raise Exception(\"Error loading page..\")\n",
    "\n",
    "    else:\n",
    "        #read the page HTML content\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        #find  all the articles on single page\n",
    "        name = soup.find('div',attrs={'class':'person-details'})\n",
    "        main_name = name.find('h1')\n",
    "#         print(main_name)\n",
    "        \n",
    "        titles = soup.find_all('li',attrs={'class':'list-result-item'})\n",
    "        \n",
    "#         print(titles)\n",
    "        print(len(titles))\n",
    "\n",
    "#         title_len = len(titles)\n",
    "    \n",
    "#         while title_len > 0: # NOT NULL\n",
    "        \n",
    "\n",
    "#             title_len -= 1\n",
    "        for title in titles:\n",
    "            year_Published = title.find('span',attrs={'class':'date'}) \n",
    "\n",
    "            try:\n",
    "                profile_name = main_name.text\n",
    "#                 print(\"Profile: \",profile_name)\n",
    "            except:\n",
    "                profile_name = False\n",
    "            pass\n",
    "                \n",
    "            try:\n",
    "                title_name = title.find('a',attrs={'class': 'link'}).text\n",
    "#                  print(\"Title: \",title_name)\n",
    "            except:\n",
    "                title_name = False\n",
    "            pass\n",
    "\n",
    "            try:\n",
    "                title_link = title.find('a')['href']\n",
    "#                  print(\"link: \",title_link)\n",
    "            except:\n",
    "                title_link = False\n",
    "            pass                       \n",
    "                \n",
    "            try:\n",
    "                year_published = year_Published.text.strip()\n",
    "#                  print(\"Year published : \", year_published)\n",
    "            except:\n",
    "                year_published = False\n",
    "            pass\n",
    "                \n",
    "                \n",
    "            article_table[document_id] = [profile_name,title_name,title_link,year_published]\n",
    "\n",
    "            document_id=document_id+1 # increment the document id for next item\n",
    "            print(\"document_id\", document_id)\n",
    "            \n",
    "#             start = start+page_size # show more pages of articles\n",
    "            \n",
    "#             if start != 100:\n",
    "#                 page_size = 80\n",
    "#             else:\n",
    "#                 page_size = 100\n",
    "\n",
    "            # new_page = requests.get(profile_url)\n",
    "            # print(\"new page url: \", new_page)\n",
    "            # soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "            # titles = soup.find_all('li',attrs={'class':'list-result-item'})\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjJ8dK9qt3nz",
    "outputId": "1ee9f794-e718-4073-9f80-164743cc5f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Test code request\n",
    "page = requests.get(seed_url)\n",
    "print(page.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiAFooevt3n0"
   },
   "source": [
    "## 1. Crawl and extract all the profile and paper related with coventry univertiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBA-pDhwt3n1",
    "outputId": "d2b61fa8-c99a-469a-fa1a-5500c4d974e9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.Crawling all the pages and extracting profile links ...\n",
      "\n",
      "Seed URL :  https://pureportal.coventry.ac.uk/en/organisations/school-of-computing-mathematics-and-data-sciences/persons/\n",
      "\n",
      "Time taken to scrape Profile data : 0.0005 sec\n",
      "\n",
      "Saving all the extracted profiles and articles into csv file..\n",
      "True\n",
      "\n",
      "Saving Completed.\n",
      "\n",
      "n 2. Crawling all the Profile links and extracting articles info ...\n",
      "\n",
      "First profile link :  https://pureportal.coventry.ac.uk/en/persons/mohamed-abdelshafy/publications/\n",
      "\n",
      "Queue length :  62\n",
      "Stop Number:  62\n",
      "12\n",
      "document_id 2\n",
      "document_id 3\n",
      "document_id 4\n",
      "document_id 5\n",
      "document_id 6\n",
      "document_id 7\n",
      "document_id 8\n",
      "document_id 9\n",
      "document_id 10\n",
      "document_id 11\n",
      "document_id 12\n",
      "document_id 13\n",
      "Processsing done , pending  61\n",
      "61,3\n",
      "document_id 14\n",
      "document_id 15\n",
      "document_id 16\n",
      "Processsing done , pending  60\n",
      "60,0\n",
      "Processsing done , pending  59\n",
      "59,31\n",
      "document_id 17\n",
      "document_id 18\n",
      "document_id 19\n",
      "document_id 20\n",
      "document_id 21\n",
      "document_id 22\n",
      "document_id 23\n",
      "document_id 24\n",
      "document_id 25\n",
      "document_id 26\n",
      "document_id 27\n",
      "document_id 28\n",
      "document_id 29\n",
      "document_id 30\n",
      "document_id 31\n",
      "document_id 32\n",
      "document_id 33\n",
      "document_id 34\n",
      "document_id 35\n",
      "document_id 36\n",
      "document_id 37\n",
      "document_id 38\n",
      "document_id 39\n",
      "document_id 40\n",
      "document_id 41\n",
      "document_id 42\n",
      "document_id 43\n",
      "document_id 44\n",
      "document_id 45\n",
      "document_id 46\n",
      "document_id 47\n",
      "Processsing done , pending  58\n",
      "58,0\n",
      "Processsing done , pending  57\n",
      "57,5\n",
      "document_id 48\n",
      "document_id 49\n",
      "document_id 50\n",
      "document_id 51\n",
      "document_id 52\n",
      "Processsing done , pending  56\n",
      "56,4\n",
      "document_id 53\n",
      "document_id 54\n",
      "document_id 55\n",
      "document_id 56\n",
      "Processsing done , pending  55\n",
      "55,0\n",
      "Processsing done , pending  54\n",
      "54,0\n",
      "Processsing done , pending  53\n",
      "53,0\n",
      "Processsing done , pending  52\n",
      "52,0\n",
      "Processsing done , pending  51\n",
      "51,25\n",
      "document_id 57\n",
      "document_id 58\n",
      "document_id 59\n",
      "document_id 60\n",
      "document_id 61\n",
      "document_id 62\n",
      "document_id 63\n",
      "document_id 64\n",
      "document_id 65\n",
      "document_id 66\n",
      "document_id 67\n",
      "document_id 68\n",
      "document_id 69\n",
      "document_id 70\n",
      "document_id 71\n",
      "document_id 72\n",
      "document_id 73\n",
      "document_id 74\n",
      "document_id 75\n",
      "document_id 76\n",
      "document_id 77\n",
      "document_id 78\n",
      "document_id 79\n",
      "document_id 80\n",
      "document_id 81\n",
      "Processsing done , pending  50\n",
      "50,3\n",
      "document_id 82\n",
      "document_id 83\n",
      "document_id 84\n",
      "Processsing done , pending  49\n",
      "49,0\n",
      "Processsing done , pending  48\n",
      "48,50\n",
      "document_id 85\n",
      "document_id 86\n",
      "document_id 87\n",
      "document_id 88\n",
      "document_id 89\n",
      "document_id 90\n",
      "document_id 91\n",
      "document_id 92\n",
      "document_id 93\n",
      "document_id 94\n",
      "document_id 95\n",
      "document_id 96\n",
      "document_id 97\n",
      "document_id 98\n",
      "document_id 99\n",
      "document_id 100\n",
      "document_id 101\n",
      "document_id 102\n",
      "document_id 103\n",
      "document_id 104\n",
      "document_id 105\n",
      "document_id 106\n",
      "document_id 107\n",
      "document_id 108\n",
      "document_id 109\n",
      "document_id 110\n",
      "document_id 111\n",
      "document_id 112\n",
      "document_id 113\n",
      "document_id 114\n",
      "document_id 115\n",
      "document_id 116\n",
      "document_id 117\n",
      "document_id 118\n",
      "document_id 119\n",
      "document_id 120\n",
      "document_id 121\n",
      "document_id 122\n",
      "document_id 123\n",
      "document_id 124\n",
      "document_id 125\n",
      "document_id 126\n",
      "document_id 127\n",
      "document_id 128\n",
      "document_id 129\n",
      "document_id 130\n",
      "document_id 131\n",
      "document_id 132\n",
      "document_id 133\n",
      "document_id 134\n",
      "Processsing done , pending  47\n",
      "47,50\n",
      "document_id 135\n",
      "document_id 136\n",
      "document_id 137\n",
      "document_id 138\n",
      "document_id 139\n",
      "document_id 140\n",
      "document_id 141\n",
      "document_id 142\n",
      "document_id 143\n",
      "document_id 144\n",
      "document_id 145\n",
      "document_id 146\n",
      "document_id 147\n",
      "document_id 148\n",
      "document_id 149\n",
      "document_id 150\n",
      "document_id 151\n",
      "document_id 152\n",
      "document_id 153\n",
      "document_id 154\n",
      "document_id 155\n",
      "document_id 156\n",
      "document_id 157\n",
      "document_id 158\n",
      "document_id 159\n",
      "document_id 160\n",
      "document_id 161\n",
      "document_id 162\n",
      "document_id 163\n",
      "document_id 164\n",
      "document_id 165\n",
      "document_id 166\n",
      "document_id 167\n",
      "document_id 168\n",
      "document_id 169\n",
      "document_id 170\n",
      "document_id 171\n",
      "document_id 172\n",
      "document_id 173\n",
      "document_id 174\n",
      "document_id 175\n",
      "document_id 176\n",
      "document_id 177\n",
      "document_id 178\n",
      "document_id 179\n",
      "document_id 180\n",
      "document_id 181\n",
      "document_id 182\n",
      "document_id 183\n",
      "document_id 184\n",
      "Processsing done , pending  46\n",
      "46,0\n",
      "Processsing done , pending  45\n",
      "45,0\n",
      "Processsing done , pending  44\n",
      "44,17\n",
      "document_id 185\n",
      "document_id 186\n",
      "document_id 187\n",
      "document_id 188\n",
      "document_id 189\n",
      "document_id 190\n",
      "document_id 191\n",
      "document_id 192\n",
      "document_id 193\n",
      "document_id 194\n",
      "document_id 195\n",
      "document_id 196\n",
      "document_id 197\n",
      "document_id 198\n",
      "document_id 199\n",
      "document_id 200\n",
      "document_id 201\n",
      "Processsing done , pending  43\n",
      "43,22\n",
      "document_id 202\n",
      "document_id 203\n",
      "document_id 204\n",
      "document_id 205\n",
      "document_id 206\n",
      "document_id 207\n",
      "document_id 208\n",
      "document_id 209\n",
      "document_id 210\n",
      "document_id 211\n",
      "document_id 212\n",
      "document_id 213\n",
      "document_id 214\n",
      "document_id 215\n",
      "document_id 216\n",
      "document_id 217\n",
      "document_id 218\n",
      "document_id 219\n",
      "document_id 220\n",
      "document_id 221\n",
      "document_id 222\n",
      "document_id 223\n",
      "Processsing done , pending  42\n",
      "42,0\n",
      "Processsing done , pending  41\n",
      "41,36\n",
      "document_id 224\n",
      "document_id 225\n",
      "document_id 226\n",
      "document_id 227\n",
      "document_id 228\n",
      "document_id 229\n",
      "document_id 230\n",
      "document_id 231\n",
      "document_id 232\n",
      "document_id 233\n",
      "document_id 234\n",
      "document_id 235\n",
      "document_id 236\n",
      "document_id 237\n",
      "document_id 238\n",
      "document_id 239\n",
      "document_id 240\n",
      "document_id 241\n",
      "document_id 242\n",
      "document_id 243\n",
      "document_id 244\n",
      "document_id 245\n",
      "document_id 246\n",
      "document_id 247\n",
      "document_id 248\n",
      "document_id 249\n",
      "document_id 250\n",
      "document_id 251\n",
      "document_id 252\n",
      "document_id 253\n",
      "document_id 254\n",
      "document_id 255\n",
      "document_id 256\n",
      "document_id 257\n",
      "document_id 258\n",
      "document_id 259\n",
      "Processsing done , pending  40\n",
      "40,13\n",
      "document_id 260\n",
      "document_id 261\n",
      "document_id 262\n",
      "document_id 263\n",
      "document_id 264\n",
      "document_id 265\n",
      "document_id 266\n",
      "document_id 267\n",
      "document_id 268\n",
      "document_id 269\n",
      "document_id 270\n",
      "document_id 271\n",
      "document_id 272\n",
      "Processsing done , pending  39\n",
      "39,40\n",
      "document_id 273\n",
      "document_id 274\n",
      "document_id 275\n",
      "document_id 276\n",
      "document_id 277\n",
      "document_id 278\n",
      "document_id 279\n",
      "document_id 280\n",
      "document_id 281\n",
      "document_id 282\n",
      "document_id 283\n",
      "document_id 284\n",
      "document_id 285\n",
      "document_id 286\n",
      "document_id 287\n",
      "document_id 288\n",
      "document_id 289\n",
      "document_id 290\n",
      "document_id 291\n",
      "document_id 292\n",
      "document_id 293\n",
      "document_id 294\n",
      "document_id 295\n",
      "document_id 296\n",
      "document_id 297\n",
      "document_id 298\n",
      "document_id 299\n",
      "document_id 300\n",
      "document_id 301\n",
      "document_id 302\n",
      "document_id 303\n",
      "document_id 304\n",
      "document_id 305\n",
      "document_id 306\n",
      "document_id 307\n",
      "document_id 308\n",
      "document_id 309\n",
      "document_id 310\n",
      "document_id 311\n",
      "document_id 312\n",
      "Processsing done , pending  38\n",
      "38,0\n",
      "Processsing done , pending  37\n",
      "37,15\n",
      "document_id 313\n",
      "document_id 314\n",
      "document_id 315\n",
      "document_id 316\n",
      "document_id 317\n",
      "document_id 318\n",
      "document_id 319\n",
      "document_id 320\n",
      "document_id 321\n",
      "document_id 322\n",
      "document_id 323\n",
      "document_id 324\n",
      "document_id 325\n",
      "document_id 326\n",
      "document_id 327\n",
      "Processsing done , pending  36\n",
      "36,0\n",
      "Processsing done , pending  35\n",
      "35,5\n",
      "document_id 328\n",
      "document_id 329\n",
      "document_id 330\n",
      "document_id 331\n",
      "document_id 332\n",
      "Processsing done , pending  34\n",
      "34,1\n",
      "document_id 333\n",
      "Processsing done , pending  33\n",
      "33,0\n",
      "Processsing done , pending  32\n",
      "32,0\n",
      "Processsing done , pending  31\n",
      "31,26\n",
      "document_id 334\n",
      "document_id 335\n",
      "document_id 336\n",
      "document_id 337\n",
      "document_id 338\n",
      "document_id 339\n",
      "document_id 340\n",
      "document_id 341\n",
      "document_id 342\n",
      "document_id 343\n",
      "document_id 344\n",
      "document_id 345\n",
      "document_id 346\n",
      "document_id 347\n",
      "document_id 348\n",
      "document_id 349\n",
      "document_id 350\n",
      "document_id 351\n",
      "document_id 352\n",
      "document_id 353\n",
      "document_id 354\n",
      "document_id 355\n",
      "document_id 356\n",
      "document_id 357\n",
      "document_id 358\n",
      "document_id 359\n",
      "Processsing done , pending  30\n",
      "30,0\n",
      "Processsing done , pending  29\n",
      "29,0\n",
      "Processsing done , pending  28\n",
      "28,0\n",
      "Processsing done , pending  27\n",
      "27,50\n",
      "document_id 360\n",
      "document_id 361\n",
      "document_id 362\n",
      "document_id 363\n",
      "document_id 364\n",
      "document_id 365\n",
      "document_id 366\n",
      "document_id 367\n",
      "document_id 368\n",
      "document_id 369\n",
      "document_id 370\n",
      "document_id 371\n",
      "document_id 372\n",
      "document_id 373\n",
      "document_id 374\n",
      "document_id 375\n",
      "document_id 376\n",
      "document_id 377\n",
      "document_id 378\n",
      "document_id 379\n",
      "document_id 380\n",
      "document_id 381\n",
      "document_id 382\n",
      "document_id 383\n",
      "document_id 384\n",
      "document_id 385\n",
      "document_id 386\n",
      "document_id 387\n",
      "document_id 388\n",
      "document_id 389\n",
      "document_id 390\n",
      "document_id 391\n",
      "document_id 392\n",
      "document_id 393\n",
      "document_id 394\n",
      "document_id 395\n",
      "document_id 396\n",
      "document_id 397\n",
      "document_id 398\n",
      "document_id 399\n",
      "document_id 400\n",
      "document_id 401\n",
      "document_id 402\n",
      "document_id 403\n",
      "document_id 404\n",
      "document_id 405\n",
      "document_id 406\n",
      "document_id 407\n",
      "document_id 408\n",
      "document_id 409\n",
      "Processsing done , pending  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26,0\n",
      "Processsing done , pending  25\n",
      "25,14\n",
      "document_id 410\n",
      "document_id 411\n",
      "document_id 412\n",
      "document_id 413\n",
      "document_id 414\n",
      "document_id 415\n",
      "document_id 416\n",
      "document_id 417\n",
      "document_id 418\n",
      "document_id 419\n",
      "document_id 420\n",
      "document_id 421\n",
      "document_id 422\n",
      "document_id 423\n",
      "Processsing done , pending  24\n",
      "24,0\n",
      "Processsing done , pending  23\n",
      "23,0\n",
      "Processsing done , pending  22\n",
      "22,25\n",
      "document_id 424\n",
      "document_id 425\n",
      "document_id 426\n",
      "document_id 427\n",
      "document_id 428\n",
      "document_id 429\n",
      "document_id 430\n",
      "document_id 431\n",
      "document_id 432\n",
      "document_id 433\n",
      "document_id 434\n",
      "document_id 435\n",
      "document_id 436\n",
      "document_id 437\n",
      "document_id 438\n",
      "document_id 439\n",
      "document_id 440\n",
      "document_id 441\n",
      "document_id 442\n",
      "document_id 443\n",
      "document_id 444\n",
      "document_id 445\n",
      "document_id 446\n",
      "document_id 447\n",
      "document_id 448\n",
      "Processsing done , pending  21\n",
      "21,2\n",
      "document_id 449\n",
      "document_id 450\n",
      "Processsing done , pending  20\n",
      "20,0\n",
      "Processsing done , pending  19\n",
      "19,0\n",
      "Processsing done , pending  18\n",
      "18,0\n",
      "Processsing done , pending  17\n",
      "17,1\n",
      "document_id 451\n",
      "Processsing done , pending  16\n",
      "16,6\n",
      "document_id 452\n",
      "document_id 453\n",
      "document_id 454\n",
      "document_id 455\n",
      "document_id 456\n",
      "document_id 457\n",
      "Processsing done , pending  15\n",
      "15,11\n",
      "document_id 458\n",
      "document_id 459\n",
      "document_id 460\n",
      "document_id 461\n",
      "document_id 462\n",
      "document_id 463\n",
      "document_id 464\n",
      "document_id 465\n",
      "document_id 466\n",
      "document_id 467\n",
      "document_id 468\n",
      "Processsing done , pending  14\n",
      "14,0\n",
      "Processsing done , pending  13\n",
      "13,0\n",
      "Processsing done , pending  12\n",
      "12,0\n",
      "Processsing done , pending  11\n",
      "11,0\n",
      "Processsing done , pending  10\n",
      "10,0\n",
      "Processsing done , pending  9\n",
      "9,48\n",
      "document_id 469\n",
      "document_id 470\n",
      "document_id 471\n",
      "document_id 472\n",
      "document_id 473\n",
      "document_id 474\n",
      "document_id 475\n",
      "document_id 476\n",
      "document_id 477\n",
      "document_id 478\n",
      "document_id 479\n",
      "document_id 480\n",
      "document_id 481\n",
      "document_id 482\n",
      "document_id 483\n",
      "document_id 484\n",
      "document_id 485\n",
      "document_id 486\n",
      "document_id 487\n",
      "document_id 488\n",
      "document_id 489\n",
      "document_id 490\n",
      "document_id 491\n",
      "document_id 492\n",
      "document_id 493\n",
      "document_id 494\n",
      "document_id 495\n",
      "document_id 496\n",
      "document_id 497\n",
      "document_id 498\n",
      "document_id 499\n",
      "document_id 500\n",
      "document_id 501\n",
      "document_id 502\n",
      "document_id 503\n",
      "document_id 504\n",
      "document_id 505\n",
      "document_id 506\n",
      "document_id 507\n",
      "document_id 508\n",
      "document_id 509\n",
      "document_id 510\n",
      "document_id 511\n",
      "document_id 512\n",
      "document_id 513\n",
      "document_id 514\n",
      "document_id 515\n",
      "document_id 516\n",
      "Processsing done , pending  8\n",
      "8,0\n",
      "Processsing done , pending  7\n",
      "7,24\n",
      "document_id 517\n",
      "document_id 518\n",
      "document_id 519\n",
      "document_id 520\n",
      "document_id 521\n",
      "document_id 522\n",
      "document_id 523\n",
      "document_id 524\n",
      "document_id 525\n",
      "document_id 526\n",
      "document_id 527\n",
      "document_id 528\n",
      "document_id 529\n",
      "document_id 530\n",
      "document_id 531\n",
      "document_id 532\n",
      "document_id 533\n",
      "document_id 534\n",
      "document_id 535\n",
      "document_id 536\n",
      "document_id 537\n",
      "document_id 538\n",
      "document_id 539\n",
      "document_id 540\n",
      "Processsing done , pending  6\n",
      "6,0\n",
      "Processsing done , pending  5\n",
      "5,0\n",
      "Processsing done , pending  4\n",
      "4,4\n",
      "document_id 541\n",
      "document_id 542\n",
      "document_id 543\n",
      "document_id 544\n",
      "Processsing done , pending  3\n",
      "3,23\n",
      "document_id 545\n",
      "document_id 546\n",
      "document_id 547\n",
      "document_id 548\n",
      "document_id 549\n",
      "document_id 550\n",
      "document_id 551\n",
      "document_id 552\n",
      "document_id 553\n",
      "document_id 554\n",
      "document_id 555\n",
      "document_id 556\n",
      "document_id 557\n",
      "document_id 558\n",
      "document_id 559\n",
      "document_id 560\n",
      "document_id 561\n",
      "document_id 562\n",
      "document_id 563\n",
      "document_id 564\n",
      "document_id 565\n",
      "document_id 566\n",
      "document_id 567\n",
      "Processsing done , pending  2\n",
      "2,0\n",
      "Processsing done , pending  1\n",
      "1,6\n",
      "document_id 568\n",
      "document_id 569\n",
      "document_id 570\n",
      "document_id 571\n",
      "document_id 572\n",
      "document_id 573\n",
      "Processsing done , pending  0\n",
      "0,\n",
      "Time taken to scrape article data : 204.7805 sec\n",
      "Nothing to process. Queue Len :  0\n",
      "No.Of Entries extracted :  62\n",
      "\n",
      "Saving all the extracted articles details into csv file..\n",
      "True\n",
      "\n",
      "Saving Completed.\n"
     ]
    }
   ],
   "source": [
    "stop = 500 # Use only for top level of scraping \n",
    "start_time = time.time() # measure time taken to scrape data \n",
    "failure=False\n",
    "\n",
    "if CRAWL_PROFILE:\n",
    "    #1.1 Crawl through all the pages and extract profile links into a list\n",
    "    print(\"\\n1.Crawling all the pages and extracting profile links ...\")\n",
    "    print(\"\\nSeed URL : \",seed_url)\n",
    "    \n",
    "    while len(queue)!=0 and stop >0:\n",
    "        try:\n",
    "            random_time=random.randint(0,10) # genrate random time wait in sec\n",
    "            #retrieve HTML content of the page\n",
    "            print(\"url : \",queue[0])\n",
    "            page =requests.get(queue[0])\n",
    "\n",
    "            #extract all the requried info from the page\n",
    "            if page.status_code==200:\n",
    "                # print(\"page-found---------------before extract_info_from_page FUNCTION\")\n",
    "                extract_info_from_page(page)\n",
    "            else:\n",
    "                print(\"Failed to load page [ERROR_CODE]: \",page.status_code)\n",
    "                print(\"Page Url : \",queue[0])\n",
    "\n",
    "            #Try next link -> pop out the first page from main queue::FIFO and add into already visited url list\n",
    "            if len(queue)>0 :\n",
    "                already_visited.append(queue.pop(0))\n",
    "            stop = stop-1\n",
    "            print(\".\",end='') # print to show that scraping is in progress...\n",
    "            time.sleep(random_time)  # add delay of 1s before visiting next page\n",
    "        except:\n",
    "            print(\"Inside level 1 crawling....something went wrong..Exiting\\n\")\n",
    "            failure=True\n",
    "            break\n",
    "\n",
    "    stop_time = time.time()        \n",
    "    if failure==False:\n",
    "        time_taken = stop_time-start_time\n",
    "        print(\"\\nTime taken to scrape Profile data :\",str(np.round(time_taken,4))+\" sec\")\n",
    "        \n",
    "        print(\"\\nSaving all the extracted profiles and articles into csv file..\")\n",
    "        #Write profile data into file\n",
    "        writeData_profile() \n",
    "        print(\"\\nSaving Completed.\")         \n",
    "    else:\n",
    "        time_taken = stop_time-start_time\n",
    "        print(\"\\nTime taken to scrape Profile data :\",str(np.round(time_taken,4))+\" sec\")\n",
    "        \n",
    "        print(\"\\nSaving all the extracted profiles and articles into csv file..\")\n",
    "        #Write profile data into file\n",
    "        writeData_profile()         \n",
    "        print(\"Level 1: Extraction of profiles failed...Saving extracted data so far. Program exit\")\n",
    "\n",
    "\n",
    "if CRAWL_ARTICLE:    \n",
    "    start_time = time.time() # measure time taken to scrape data \n",
    "    #1.2 Crawl through all the profiles link now to extract articles for each users\n",
    "    profile_df =pd.read_csv(\"CoventryUni_Data_Scraped_profile.csv\",header='infer')\n",
    "    profile_queue = profile_df['Profile_Link'].values # assign the profile_queue with list of all the pofile link ::FIFO\n",
    "    head_profile_url = profile_queue[0]\n",
    "    stop = len(profile_queue)\n",
    "\n",
    "    print(\"\\nn 2. Crawling all the Profile links and extracting articles info ...\")\n",
    "    print(\"\\nFirst profile link : \",head_profile_url)\n",
    "    print(\"\\nQueue length : \",len(profile_queue))\n",
    "    print(\"Stop Number: \", stop)\n",
    "\n",
    "    while len(profile_queue) != 0 and stop >0:\n",
    "        try:\n",
    "            stop = stop-1\n",
    "            random_time=random.randint(0,1) # genrate random time wait in sec\n",
    "            \n",
    "#             print(\"Before Extract Function\")\n",
    "\n",
    "               # Crawl to individual user profile and extract their published articles details\n",
    "            extract_user_info(head_profile_url)\n",
    "            print(\"Processsing done , pending \", len(profile_queue)-1)\n",
    "                #pop out the first link from the queue and assign second link as first item\n",
    "            if len(profile_queue)!=1:\n",
    "                profile_queue = profile_queue[1:]\n",
    "                #assign head of queue to head_profile_url for next crawl\n",
    "                head_profile_url = profile_queue[0]\n",
    "            else:\n",
    "                profile_queue = [] # All the links are crawled\n",
    "\n",
    "            print(len(profile_queue),end=',') # print to show that scraping is in progress...\n",
    "            time.sleep(random_time)  # add delay of 1s before visiting next page\n",
    "        except:\n",
    "            print(\"Inside level 2 crawling....something went wrong..Exiting\\n\")\n",
    "            failure = True\n",
    "            break\n",
    "\n",
    "    stop_time = time.time()\n",
    "    if failure == False:\n",
    "        time_taken = stop_time-start_time\n",
    "        print(\"\\nTime taken to scrape article data :\",str(np.round(time_taken,4))+\" sec\")\n",
    "\n",
    "        print(\"Nothing to process. Queue Len : \",len(queue))\n",
    "        print(\"No.Of Entries extracted : \", total_entries)\n",
    "\n",
    "        #Write published article data into file\n",
    "        print(\"\\nSaving all the extracted articles details into csv file..\")    \n",
    "        writeData_articles()\n",
    "        print(\"\\nSaving Completed.\")  \n",
    "    else:\n",
    "        print(\"pages pending to crawl : \",len(profile_queue))\n",
    "        time_taken = stop_time-start_time\n",
    "        print(\"\\nTime taken to scrape article data :\",str(np.round(time_taken,4))+\" sec\")        \n",
    "        writeData_articles()\n",
    "        print(\"Level 2: Extraction of articles failed.. Saving crawled data so far.. Program exit\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V3HQFmMYt3n1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
